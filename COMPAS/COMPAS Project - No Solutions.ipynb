{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Fairness: Considering Different Definitions\n",
    "\n",
    "Approximate notebook time: 2 hours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Decision making within the United States criminal justice system relies heavily on risk assessment, which determines the potential risk that a released defendant will fail to appear in court or cause harm to the public. Judges use these assessments to decide if bail can be set or if a defendant should be detained before trial. While this is not new in the legal system, the use of risk scores determined by an algorithm are gaining prevalence and support. Proponents promote the use of risk scores to guide judges in their decision making, arguing that machine learning could lead to greater efficiency, accountability, and less biased decisions compared with human judgment ([Henry](https://theappeal.org/risk-assessment-explained/)). On the other hand, critical voices raise the concern that biases can creep into these algorithms at any point in the process, and that algorithms are often applied to the wrong situations ([Henry](https://theappeal.org/risk-assessment-explained/)). Further, they exacerbate the racism embedded deep within the criminal justice system by perpetuating inequalities found in historical data ([Henry](https://theappeal.org/risk-assessment-explained/)).\n",
    "\n",
    "In the debate about the use of risk assessment algorithms, people have used data analysis to determine the extent to which these algorithms are fair to different groups of people. In this homework, **you will explore some of the many definitions and metrics (different ways of operationalizing data to quantify those definitions) of fairness that can be applied to the risk assessment tool COMPAS**. In doing so, you will understand and provide evidence for or against the presence of bias within the algorithm. You will examine the arguments and analyses made by the company that created COMPAS and the critics of this risk assessment tool to gain a deeper understanding of the technical and societal interpretations and implications of fairness. \n",
    "\n",
    "**NOTE**: When we discuss bias in this module, we define it most generally as prejudice or an inclination in favor of one person, thing, or group compared to another. In the context of machine learning, bias is a “phenomenon that occurs when an algorithm produces results that are systemically prejudiced due to erroneous assumptions in the machine learning process” ([Rouse](https://searchenterpriseai.techtarget.com/definition/machine-learning-bias-algorithm-bias-or-AI-bias#:~:text=Machine%20learning%20bias%2C%20also%20sometimes,in%20the%20machine%20learning%20process))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Part 0. COMPAS](#part-zero)\n",
    "* [Part 1. ProPublica's Perspective](#part-one)\n",
    "* [Part 2. Northpointe's Perspective](#part-two)\n",
    "* [Part 3. Another Definition of Fairness](#part-three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's begin by importing the packages we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "\"hide-output\""
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aif360 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from aif360) (1.18.5)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from aif360) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from aif360) (0.23.1)\n",
      "Requirement already satisfied: matplotlib in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from aif360) (3.2.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from aif360) (1.0.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21->aif360) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21->aif360) (2.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->aif360) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->aif360) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->aif360) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->aif360) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.0->aif360) (2020.1)\n",
      "Requirement already satisfied: six in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->aif360) (1.15.0)\n",
      "Requirement already satisfied: BlackBoxAuditing in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (0.1.54)\n",
      "Requirement already satisfied: pandas in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from BlackBoxAuditing) (1.0.5)\n",
      "Requirement already satisfied: matplotlib in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from BlackBoxAuditing) (3.2.2)\n",
      "Requirement already satisfied: networkx in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from BlackBoxAuditing) (2.4)\n",
      "Requirement already satisfied: numpy in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from BlackBoxAuditing) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from pandas->BlackBoxAuditing) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from pandas->BlackBoxAuditing) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from networkx->BlackBoxAuditing) (4.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/eva/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas->BlackBoxAuditing) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "!pip install aif360\n",
    "!pip install BlackBoxAuditing\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.datasets import BinaryLabelDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0. COMPAS: Why it was created and how it exists in the court system <a id=\"part-zero\"></a>\n",
    "\n",
    "COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a commercial tool produced by the for-profit company Northpointe (acquired by equivant) known as a recidivism risk assessment system. **Tools like COMPAS are used to predict the risk of future crimes for an individual who has entered the US criminal justice system by outputting a risk score from 1-10**. While COMPAS was initially intended to aid decisions made by probation officers on treatment and supervision of those who are incarcerated, Northpointe has since emphasized the scalability of the tool to “fit the needs of many different decision points” including pre-screening assessments, pretrial release decisions (whether or not to hold an arrested individual in jail until their trial), and post-trial next steps for the defendant ([Northpointe](http://www.northpointeinc.com/files/downloads/FAQ_Document.pdf)). These algorithms are believed by many to provide the ability to make the court system more just by removing or correcting for bias of criminal justice officials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0a\n",
    "Explain 3 parties that are impacted by the COMPAS tool. In what ways are they impacted? (Can you think of impacts beyond those in the courtroom for at least one of your examples?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0b\n",
    "Based on your initial reading, what is one problem of the criminal justice system that the COMPAS tool could potentially alleviate? What is one potential problem that using the COMPAS algorithm could introduce? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "We will be using the data that was obtained and used by ProPublica in their own analysis of the COMPAS tool from Broward County public records of people who were scored between 2013 and 2014 ([ProPublica](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)). In order to replicate ProPublica's analysis, we remove any cases where the charge was not within 30 days of the score (ProPublica did this in order to match the COMPAS score with the correct criminal case). We are left with 6172 rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>...</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>miguel hernandez</td>\n",
       "      <td>miguel</td>\n",
       "      <td>hernandez</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1947-04-18</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>kevon dixon</td>\n",
       "      <td>kevon</td>\n",
       "      <td>dixon</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>1982-01-22</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ed philo</td>\n",
       "      <td>ed</td>\n",
       "      <td>philo</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-14</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>marsha miles</td>\n",
       "      <td>marsha</td>\n",
       "      <td>miles</td>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>Male</td>\n",
       "      <td>1971-08-22</td>\n",
       "      <td>44</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>2013-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>edward riddle</td>\n",
       "      <td>edward</td>\n",
       "      <td>riddle</td>\n",
       "      <td>2014-02-19</td>\n",
       "      <td>Male</td>\n",
       "      <td>1974-07-23</td>\n",
       "      <td>41</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-02-19</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>10996</td>\n",
       "      <td>steven butler</td>\n",
       "      <td>steven</td>\n",
       "      <td>butler</td>\n",
       "      <td>2013-11-23</td>\n",
       "      <td>Male</td>\n",
       "      <td>1992-07-17</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-11-23</td>\n",
       "      <td>2013-11-22</td>\n",
       "      <td>2013-11-24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>10997</td>\n",
       "      <td>malcolm simmons</td>\n",
       "      <td>malcolm</td>\n",
       "      <td>simmons</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>Male</td>\n",
       "      <td>1993-03-25</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>2014-02-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7211</th>\n",
       "      <td>10999</td>\n",
       "      <td>winston gregory</td>\n",
       "      <td>winston</td>\n",
       "      <td>gregory</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1958-10-01</td>\n",
       "      <td>57</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>2014-01-13</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>808</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>11000</td>\n",
       "      <td>farrah jean</td>\n",
       "      <td>farrah</td>\n",
       "      <td>jean</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>Female</td>\n",
       "      <td>1982-11-17</td>\n",
       "      <td>33</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>2014-03-08</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>754</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>11001</td>\n",
       "      <td>florencia sanmartin</td>\n",
       "      <td>florencia</td>\n",
       "      <td>sanmartin</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>Female</td>\n",
       "      <td>1992-12-18</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2015-03-15</td>\n",
       "      <td>2015-03-15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6172 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                 name      first       last compas_screening_date  \\\n",
       "0         1     miguel hernandez     miguel  hernandez            2013-08-14   \n",
       "1         3          kevon dixon      kevon      dixon            2013-01-27   \n",
       "2         4             ed philo         ed      philo            2013-04-14   \n",
       "5         7         marsha miles     marsha      miles            2013-11-30   \n",
       "6         8        edward riddle     edward     riddle            2014-02-19   \n",
       "...     ...                  ...        ...        ...                   ...   \n",
       "7209  10996        steven butler     steven     butler            2013-11-23   \n",
       "7210  10997      malcolm simmons    malcolm    simmons            2014-02-01   \n",
       "7211  10999      winston gregory    winston    gregory            2014-01-14   \n",
       "7212  11000          farrah jean     farrah       jean            2014-03-09   \n",
       "7213  11001  florencia sanmartin  florencia  sanmartin            2014-06-30   \n",
       "\n",
       "         sex         dob  age          age_cat              race  ...  \\\n",
       "0       Male  1947-04-18   69  Greater than 45             Other  ...   \n",
       "1       Male  1982-01-22   34          25 - 45  African-American  ...   \n",
       "2       Male  1991-05-14   24     Less than 25  African-American  ...   \n",
       "5       Male  1971-08-22   44          25 - 45             Other  ...   \n",
       "6       Male  1974-07-23   41          25 - 45         Caucasian  ...   \n",
       "...      ...         ...  ...              ...               ...  ...   \n",
       "7209    Male  1992-07-17   23     Less than 25  African-American  ...   \n",
       "7210    Male  1993-03-25   23     Less than 25  African-American  ...   \n",
       "7211    Male  1958-10-01   57  Greater than 45             Other  ...   \n",
       "7212  Female  1982-11-17   33          25 - 45  African-American  ...   \n",
       "7213  Female  1992-12-18   23     Less than 25          Hispanic  ...   \n",
       "\n",
       "      v_decile_score  v_score_text  v_screening_date  in_custody  out_custody  \\\n",
       "0                  1           Low        2013-08-14  2014-07-07   2014-07-14   \n",
       "1                  1           Low        2013-01-27  2013-01-26   2013-02-05   \n",
       "2                  3           Low        2013-04-14  2013-06-16   2013-06-16   \n",
       "5                  1           Low        2013-11-30  2013-11-30   2013-12-01   \n",
       "6                  2           Low        2014-02-19  2014-03-31   2014-04-18   \n",
       "...              ...           ...               ...         ...          ...   \n",
       "7209               5        Medium        2013-11-23  2013-11-22   2013-11-24   \n",
       "7210               5        Medium        2014-02-01  2014-01-31   2014-02-02   \n",
       "7211               1           Low        2014-01-14  2014-01-13   2014-01-14   \n",
       "7212               2           Low        2014-03-09  2014-03-08   2014-03-09   \n",
       "7213               4           Low        2014-06-30  2015-03-15   2015-03-15   \n",
       "\n",
       "      priors_count.1 start  end event two_year_recid  \n",
       "0                  0     0  327     0              0  \n",
       "1                  0     9  159     1              1  \n",
       "2                  4     0   63     0              1  \n",
       "5                  0     1  853     0              0  \n",
       "6                 14     5   40     1              1  \n",
       "...              ...   ...  ...   ...            ...  \n",
       "7209               0     1  860     0              0  \n",
       "7210               0     1  790     0              0  \n",
       "7211               0     0  808     0              0  \n",
       "7212               3     0  754     0              0  \n",
       "7213               2     0  258     0              1  \n",
       "\n",
       "[6172 rows x 53 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('compas-scores-two-years.csv')\n",
    "data = data.query('days_b_screening_arrest <= 30 & days_b_screening_arrest >= -30')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also able to filter out any information that was not used by ProPublica and select fields for severity of charge, number of priors, demographics, age, sex, compas scores, and whether each person was accused of a crime within two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_data = data[[\"age\", \"c_charge_degree\", \"race\", \"age_cat\", \"score_text\", \"sex\", \"priors_count\", \n",
    "                    \"days_b_screening_arrest\", \"decile_score\", \"is_recid\", \"two_year_recid\", \"c_jail_in\", \"c_jail_out\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0c\n",
    "Explore the dataset. What is the granularity of this dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Sensitive features*** are attributes within a dataset that are given special consideration and treatment for potential legal, social, or ethical reasons. Often, these features are recognized and protected by antidiscrimination or privacy laws. One example of a sensitive feature is age. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0d\n",
    "Identify 2 sensitive features in the dataset that we have not already mentioned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0e \n",
    "Pick one of the sensitive features you have identified. What features are proxies for that sensitive feature? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0f\n",
    "As a data scientist, why is it important to give special consideration to these kinds of features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. ProPublica’s Perspective <a id=\"part-one\"></a>\n",
    "\n",
    "### Who is ProPublica?\n",
    "\n",
    "ProPublica is a nonprofit organization that “produces investigative journalism with moral force” ([ProPublica](https://www.propublica.org/about/)). ProPublica was founded as a nonpartisan newsroom aiming to expose and question abuses of power, justice, and public trust, often by systems and institutions deeply ingrained in the US.\n",
    "\n",
    "In 2016, ProPublica investigated the COMPAS algorithm to assess the accuracy of and potential racial bias within the tool, as it became more popular within the United States court system nationwide. In their analysis, ProPublica used data from defendants with risk scores from Broward County, FL from 2013 to 2014 to test for statistical differences in outcomes for Black and white defendants, which ultimately highlighted racial disparities that exist within the algorithm. ProPublica came to the conclusion that COMPAS utilizes data from a criminal justice system with a history of racial injustices, thus continuing to disproportionately target and arrest Black people in comparison to their white counterparts. While the COMPAS algorithm’s treats unequal groups alike, which may appear neutral, ProPublica’s data analysis and reporting emphasized the bias against Black defendants and their communities that COMPAS produced from this line of thinking, a claim that Northpointe has disputed (as we will see later).\n",
    "\n",
    "Let's retrace ProPublica's statistical analysis in order to better understand ProPublica's argument and engage with the metric of fairness that it uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. Logistic Regression: What are the odds of getting a high risk score?\n",
    "\n",
    "ProPublica’s first attempt at understanding the disparity in risk scores from the COMPAS tool was through logistic regression to model the chance of getting a “higher” (i.e. more \"risky\") score. COMPAS labels scores 1-4 as low, 5-7 as medium, and 8-10 as high scores. For the purposes of their analysis, ProPublica labeled any score above a low score as high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1a (i)\n",
    "Create a logistic regression model to predict the score of defendants based on their sex, age, race, previous arrests, seriousness of the crime, and future criminal behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression model\n",
    "# Student Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1a (ii)\n",
    "Print out the coefficients paired with the corresponding feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair the coefficients with feature names\n",
    "# Student Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1b\n",
    "What features are most predictive? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1c\n",
    "Are Black defendants more likely to get a high risk score opposed to white defendants? If so, by how much? Show your calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Calculations Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. FPR and FNR: Does COMPAS overpredict or underpredict across groups?\n",
    "\n",
    "In order to answer this question and understand the ways in which bias is present in the risk scores, ProPublica used the False Positive Rate (FPR) and False Negative Rate (FNR) as their metrics to understand and quantify fairness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2a\n",
    "Complete the following functions to calculate the FPR and FNR. Afterwards, apply these functions to each racial subgroup: Black defendants and white defendants. Keep in mind that ProPublica defines a high score as anything above 4, and therefore a false positive would be a defendant with a high score who did not recidivate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(race_feature, data):\n",
    "    # Return the False Positive Rate of scores for the specified race_feature\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fnr(race_feature, data):\n",
    "    # Return the False Negative Rate of scores for the specified race_feature\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR for Black defendants: 0\n",
      "FPR for white defendants: 0\n",
      "FNR for Black defendants: 0\n",
      "FNR for white defendants: 0\n"
     ]
    }
   ],
   "source": [
    "# Apply the metrics to the dataset - Replace zeros with correct metrics\n",
    "print(\"FPR for Black defendants:\", 0)\n",
    "print(\"FPR for white defendants:\", 0)\n",
    "print(\"FNR for Black defendants:\", 0)\n",
    "print(\"FNR for white defendants:\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2b\n",
    "What can you conclude from these metrics about the overprediction of risk scores for Black and white defendants? By how much is the tool overpredicting?  (Hint: Look at your calculations for the FPR.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of ratio for FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2c\n",
    "What can you conclude from these metrics about the underprediction of risk scores for Black and white defendants? By how much is the tool underpredicting? (Hint: Look at your calculations for the FNR.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of ratio for FNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2d\n",
    "What is the importance of overprediction and underprediction in regard to ProPublica’s analysis? How might these observations have real impacts on the defendants who receive scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a (i)\n",
    "Utilizing your answers from 1b and 2b, what problems does ProPublica highlight in the COMPAS algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a (ii)\n",
    "How would you describe ProPublica’s definition of fairness, after learning and utilizing the metrics they used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b \n",
    "Why did ProPublica choose to investigate bias between races rather than a different sensitive feature? (Hint: think about how ProPublica’s conclusions reflect the racial disparities in our current criminal justice system.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3c\n",
    "What is ProPublica’s agenda as an investigative journalism organization? How do we see this in their analysis and conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that Northpointe disagreed with ProPublica's argument that the COMPAS algorithm is racially biased. Now that we’ve analyzed ProPublica’s perspective and seen the way in which they define and operationalize the concept of fairness, let’s move on to Northpointe’s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Northpointe's Perspective <a id=\"part-two\"></a>\n",
    "\n",
    "### Who is Northpointe? \n",
    "\n",
    "Northpointe (merged with two other companies to create *equivant* in 2017) is a for-profit computer software company that aims to advance justice by informing and instilling confidence in decision makers at every stage of the criminal justice system ([equivant](https://www.equivant.com/)). In addition to operating and continuing to develop COMPAS, *equivant* has developed a variety of technologies for use in court case management, attorney case management, inmate classification, and risk/needs assessment strategies. \n",
    "\n",
    "In the wake of criticism from ProPublica and other researchers alike, Northpointe produced a [detailed response](http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf) to ProPublica’s allegations, claiming that these critiques of their tool utilized the wrong type of classification statistics in their analysis and portrayed the tool incorrectly. The company provided their own analysis of the COMPAS algorithm by using different statistical methods and responding individually to each of ProPublica’s claims of racial bias against Black defendants. \n",
    "\n",
    "Upon examining their tool’s fairness through accuracy equity and predictive parity (which are metrics that were left out of ProPublica’s analysis), as well as the fact that the model was not trained with a race feature, Northpointe concluded that their algorithm treats all citizens and specified groups equally, and therefore does not exhibit signs of bias or inequality for specified groups. Now, let’s take a look at how Northpointe supported this argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. Accuracy Equity: Is each group being discriminated against equally?\n",
    "\n",
    "Instead of analyzing and comparing the model errors FNR and FPR, Northpointe utilized the complement of FNR, known as the TPR (or what is often referred to as *Sensitivity*), paired with the FPR to prove what they refer to as ***Accuracy Equity*** through the use of a *ROC Curve*. Accuracy equity, according to [Northpointe](http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf), is exhibited in the model “if it can discriminate recidivists and nonrecidivists equally well for two different groups such as blacks and whites.” Recall that we use ROC curves and the *Area Under the Curve* to understand how much a model is capable of distinguishing between classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4a\n",
    "Utilize the sklearn metrics package to calculate TPR and FPR, visualize the ROC curve for both white and Black defendants, and calculate the AUC for each curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate FPR and FNR from metrics package - Black defendants\n",
    "\n",
    "# Calculate FPR and FNR from metrics package - White defendants\n",
    "\n",
    "# Plot the ROC \n",
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.title('ROC - Black Defendents')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for Black defendants: 0\n",
      "AUC for white defendants: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the AUC - Replace zeros with correct AUC's\n",
    "print(\"AUC for Black defendants:\", 0)\n",
    "print(\"AUC for white defendants:\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b (i)\n",
    "What do you notice from the ROC curve and the AUC calculation? List at least two general observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b (ii)\n",
    "What could Northpointe take away from this visualization to prove their point? Is accuracy equity being represented here? (Hint: Is each racial group being discriminated against equally?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. Predictive Parity: Is the likelihood of recidivism equal across groups?\n",
    "\n",
    "In addition to the metric outlined above, Northpointe also utilized positive predictive values to explore the likelihood of defendants to reoffend, and to therefore prove that ***Predictive Parity*** is achieved. Predictive parity, according to [Northpointe](http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf), is exhibited in a model “if the classifier obtains similar predictive values for two different groups such as blacks and whites, for example, the probability of recidivating, given a high risk score, is similar for blacks and whites.” Let’s explore how they analyzed this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5a \n",
    "\n",
    "Complete the following functions to calculate the positive predictive values and negative predictive values. Afterwards, apply these functions to the data of white defendants and the data of Black defendants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppv(race_feature, data):\n",
    "    # Return the Positive Predictive Value of scores for the specified race_feature\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def npv(race_feature, data):\n",
    "    # Return the Negative Predictive Value of scores for the specified race_feature\n",
    "    \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPV for Black defendants: 0\n",
      "NPV for Black defendants: 0\n",
      "PPV for white defendants: 0\n",
      "NPV for white defendants: 0\n"
     ]
    }
   ],
   "source": [
    "# Apply metrics to the dataset - Replace zeros with correct metrics\n",
    "print(\"PPV for Black defendants:\", 0)\n",
    "print(\"NPV for Black defendants:\", 0)\n",
    "print(\"PPV for white defendants:\", 0)\n",
    "print(\"NPV for white defendants:\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5b\n",
    "Use the metrics you calculated above to fill in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  White  | Black    |\n",
    "|---:|:-------------|:-----------|\n",
    "| Labeled higher risk, but didn't reoffend | % | % |\n",
    "| Labeled lower risk, but did reoffend | % | % |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student calculations for above table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5c (i)\n",
    "What do you notice about the positive predictive values for each group? List at least one general observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5c (ii)\n",
    "What could Northpointe conclude from these findings? Is predictive parity represented here? (Hint: Is the likelihood of recidivism relatively equal for each racial group?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6a\n",
    "How would you describe Northpointe’s definition of fairness, after learning and utilizing the metrics they used? How is this different from your description of ProPublica’s definition from Q3aii? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6b \n",
    "\n",
    "If anything, what are ProPublica and Northpointe each not considering in their definitions? (Hint: Think about other goodness metrics in ML, as well as your knowledge of the historical context of policing data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we’ve investigated ProPublica’s and Northpointe’s definitions of fairness. In the world of machine learning there are [many more](https://www.google.com/url?q=https://fairmlbook.org/tutorial2.html&sa=D&ust=1606727018134000&usg=AOvVaw06zU_fm8h7xp71d8igA8KI), so in the next section we will take a look at a third definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Yet Another Definition of Fairness <a id=\"part-three\"></a>\n",
    "\n",
    "In this section, you will go through yet another metric and definition used to evaluate fairness in machine learning: **disparate impact**. Disparate impact is a legal doctrine that determines if there is unintended discrimination towards a protected class ([Society for Human Resource Management](https://www.shrm.org/resourcesandtools/tools-and-samples/hr-qa/pages/disparateimpactdisparatetreatment.aspx)). In machine learning, disparate impact is a metric to evaluate fairness in a model. It is a form of bias within an algorithm that reflects systemic discrimination when a model’s outputs are dependent on a ***sensitive feautre*** (the protected class). This is often considered unintentional (like the legal doctrine) due to the fact that the sensitive feature is omitted from the model, though it is still correlated with the output through proxy variables ([Wang et al.](https://arxiv.org/pdf/1801.05398.pdf#:~:text=Abstract%E2%80%94In%20the%20context%20of,e.g.%2C%20race%20or%20gender)).\n",
    "\n",
    "Not only will you evaluate the fairness of the tool (as Northpointe and ProPublica did) by measuring the bias reflected in the outputs of the model, but you will remove it to actually change those outputs and therefore eliminate the dependencies between the risk scores and the race feature. In order to computationally remove the disparate impact that we quantify, we can use tools like aif360’s [Disparate Impact Remover](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html). aif360 is a package created by IBM’s AI Research team, which contains a variety of tools to “help you examine, report, and mitigate discrimination and bias in machine learning models throughout the AI application lifecycle” ([AI Fairness 360](https://aif360.mybluemix.net/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7. Disparate Impact: Quantification and Removal\n",
    "\n",
    "First, let’s visualize the disparity that we would like to remove from the dataset. In order to do that we need to distinguish between a privileged group and an unprivileged group. In technical terms, the privileged group receives higher scores from a trained model, so therefore the Black defendants will be considered \"privileged\" and the white defendants will be considered \"unprivileged\" in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7a\n",
    "\n",
    "Use a histogram to plot the scores for Black defendants and the scores for white defendants. Visualize both histograms on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Code for Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7b \n",
    "\n",
    "What do you notice from the plot? (Hint: how do the distributions differ across racial groups with respect to mean, shape of distribution, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to quantify the disparate impact we are seeing in the plot. In machine learning, we can understand disparate impact as the proportion of individuals that get positive outcomes (did they get a high score) for the two groups described above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Pr(Y=1|D=Unprivileged) \\ / \\ \\Pr(Y=1|D=Privileged)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation Y is 1 if the defendant received a high score and 0 if they received a low score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7c (i)\n",
    "\n",
    "Create a function to calculate the proportion of individuals from a specified racial group that get positive outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion(data, group):\n",
    "    # Returns the proportion of individuals in data from the group who recidivate\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7c (ii)\n",
    "\n",
    "Use this function to calculate the disparate impact, using the equation from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the proportion of unprivileged individuals receiving positive outcomes to privileged individuals receiving positive outcomes is less than 80%, there is a disparate impact violation. In order to stop a trained model from replicating these biases in its output, we can now use aif360’s Disparate Impact Remover to remove the bias we just calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7d\n",
    "\n",
    "Create a Disparate Impact Remover type object and use the function fit_transform on our data. In order to do this you will need to first create a BinaryLabelDataset from our dataset to use for fit_transform. Check out the documentation [here](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset) to see how to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame with just the necessary columns - Only numeric values for race, decile score, and two year recid\n",
    "\n",
    "# The BinaryLabelDataset requires decile_scores to be continuous -->\n",
    "# Use this line of code noise = np.random.normal(0, 0.1, race_data.shape[0]) to add noise to your decile_score column\n",
    "# Student Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7e\n",
    "\n",
    "Similar to part a, use a histogram to plot the scores on the modified dataset. Afterwards, use the proportion function created above to calculate the disparate impact of the transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform output from DIRemover into usable DataFrame\n",
    "\n",
    "# Plot the new overlayed histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Calculation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7f\n",
    "\n",
    "What has changed from our original histogram? Please explain why this change has happened. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7g\n",
    "\n",
    "How might we understand this definition of fairness, after learning and utilizing these metrics? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7h\n",
    "How does this definition of fairness differ from ProPublica’s and Northpointe’s? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8. Considering Expertise Outside of Data Science\n",
    "\n",
    "Just now, you used your technical data science skills to computationally remove bias from the data set. By removing bias, we’ve made the outputs of the algorithm statistically fair in regards to one definition of fairness. However, it is important to consider many types of knowledge and experiences beyond data science expertise when analyzing and creating an algorithm like COMPAS. As such, you will think through issues of expertise and fairness in the next set of questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8a\n",
    "\n",
    "Look back to your answer from Q0a. Now that you’ve gone through several definitions of fairness, how would you add to or revise your answer: Explain 3 parties that are impacted by the COMPAS tool. In what ways are they impacted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8b\n",
    "\n",
    "What expertise and lived experiences are necessary to understand and critically think about the issues produced by COMPAS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8c\n",
    "\n",
    "Why is this third definition of fairness still inadequate as a measurement of justice in the court system? (Hint: look at the previous two questions and answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9. Which Definition Is Fair? And Who Decides?\n",
    "\n",
    "We’ve now gone through three definitions of fairness, each one with a different idea of how to operationalize fairness and to judge whether or not an algorithm is fair. As a data scientist, you may encounter situations where you will need to make decisions that affect real-world outcomes and people! Let’s try to do this for COMPAS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9a\n",
    "If you had to decide between three definitions of fairness above, which definition do you think would make “fair” decisions for everyone who goes through the court system? What values did you consider as you made this decision? If you cannot come to a decision, what challenges did you come across when considering this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9b\n",
    "Take a step back and think about how different actors who created, utilize, and are affected by COMPAS would consider which definition is most fair. Name two relevant actors, and discuss what they would value in *their own* definitions of fairness. Of the three definitions you have explored, which would they decide is most fair from the perspective of that actor? If you don’t think they’d choose any of them, explain why. (Examples of actors, which you’re welcome to use: judges, defendants, police, policy makers, community members) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing one definition of fairness can be incredibly difficult when you need to consider all the actors at play. Throughout this module we have examined where and how the COMPAS algorithm is appropriate to use. It is also important to recognize the problems that are not solvable by an algorithm and think through how we can make the ecosystem that COMPAS is in  (which includes but is not limited to the legal system, affected communities, the tech industry, etc.) more equitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9c\n",
    "What issues that are relevant to the COMPAS ecosystem but outside of the algorithm itself need to be addressed to be able to create a more equitable system, with or without the algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student Written Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve now begun to think through the very complex systems in which the COMPAS algorithm functions. **Congratulations!** Through considering a few of the differing definitions of fairness connected to COMPAS, hopefully you can begin to understand some of the human contexts of  creating algorithms that intentionally affect people and their decision-making. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
