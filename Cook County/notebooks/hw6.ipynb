{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('hw6.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6: Exploring fairness through Cook County’s property assessments\n",
    "\n",
    "## Due Date: 11:59pm Monday, April 6\n",
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the collaborators cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *write names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This assignment will continue from where we left off in in Homework 5. Recall that the linear model that you created failed to produce accurate estimates of the observed housing prices because the model was too simple. The goal of this homework is to guide you through the iterative process of specifying, fitting, and analyzing the performance of more complex linear models used to predict prices of houses in Cook County, Illinois. Additionally, you will have the opportunity to choose your own features and create your own regression model!\n",
    "\n",
    "By the end of this homework, you should feel comfortable:\n",
    "\n",
    "1. Identifying informative variables through EDA\n",
    "2. Feature engineering categorical variables\n",
    "3. Using sklearn to build more complex linear models\n",
    "\n",
    "Additionally, as a continuation of the last homework, we’ll explore the dynamics of the CCAO’s appraisal system with more depth as you continue developing your housing prediction model. Alongside our discussion on implicit bias, however, we’ll tackle another central facet of the CCAO’s work: transparency. \n",
    "\n",
    "As you work through this assignment, consider the balance of power between the CCAO and its constituents - how might transparency redistribute this balance? And what are the limits of transparency as a solution for systemic inequity?\n",
    "\n",
    "## HCE Learning Outcomes\n",
    "\n",
    "Through the completion of this homework, student will be able to:\n",
    "* Understand the relationship between bias and fairness.\n",
    "* Analyze the technical and performative functions of transparency initiatives.\n",
    "* Recognize the social aspects of transparency in regard to the redistribution of power between different stakeholders.\n",
    "* Weigh the effectiveness and limitations of transparency as a means of arriving at fair algorithmic systems in order to reimagine equitable practices in data science.\n",
    "\n",
    "\n",
    "## Score Breakdown\n",
    "\n",
    "*To be determined by course staff*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "As a reminder, the Cook County dataset contains residential sales data that the CCAO uses to assess property values. A more detailed description of each variable is available on [their website](https://datacatalog.cookcountyil.gov/Property-Taxation/Cook-County-Assessor-s-Residential-Sales-Data/5pge-nu6u). There, the list of column values notes what each entry represents and sometimes cautions about the quality of a given variable. There are 83 features in total.\n",
    "\n",
    "The raw data are split into training and test sets with 2000 and 930 observations, respectively. To save some time, we've used a slightly modified data cleaning pipeline from last week's assignment to prepare the training data. This data is stored in `ccao_train_cleaned.csv`. It consists of 1998 observations and 83 features (we added TotalBathrooms from Homework 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"ccao_train_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Fairness\n",
    "\n",
    "When the use of algorithms and statistical modeling has real-world consequences, we often refer to the idea of fairness as a measurement of how socially-responsible our work is. Does our algorithm make similar predictions across race and gender identities? Does our model adequately represent reality?\n",
    "\n",
    "In the case of the Cook County Assessor’s Office, fair property tax rates are contingent on whether property values are assessed accurately - that they’re valued at what they’re worth, relative to properties with similar characteristics. This implies that having a more accurate model results in  fairer assessments. The goal of creating a property assessment model, then, is to be as accurate as possible, which is typically approached by attempting to minimize human-related biases in the data collection and modeling process. \n",
    "\n",
    "In our previous examination of bias through Homework 1, however, we established that, because of human involvement and historical/institutional contexts, bias is impossible to eradicate. This adds a new dimension to our understanding of fairness in home assessments: Having “accurate” (and therefore fair) assessments requires us to constantly reflect on the decisions we make throughout the data lifecycle, as well as the contexts in which we’re working. Keep this relationship between bias and fairness in mind as we continue building our model!\n",
    "\n",
    "## HCE: Question -1\n",
    "\n",
    "Based on your work in the previous homework, what would you define as a fair property assessment? There isn’t one right answer! Share your thoughts in 1-2 sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HCE: Question 0\n",
    "\n",
    "Does removing human judgment from the assessment process make assessments fairer? In what ways? How might it be (still) unfair?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: More Feature Selection and Engineering\n",
    "\n",
    "In this section, we identify two more features of the dataset that will increase our linear regression model's accuracy. Additionally, we will implement one-hot encoding so that we can include binary and categorical variables in our improved model.Before we make changes to our basic linear model, let’s go back to the data. In this section, we’ll identify two more features of the dataset that will increase our linear regression model's accuracy. Additionally, we will implement one-hot encoding so we can include binary and categorical variables in our improved model.\n",
    "\n",
    "We’ll start by first diving into the hallmark of the CCAO’s existing model: mass appraisal.\n",
    "\n",
    "### Mass Appraisal\n",
    "\n",
    "A unique technique employed by the current assessor’s office is the idea of mass appraisal. Rather than assessing homes one by one, mass appraisal evaluates value by looking to the real estate market for local trends based on location and property characteristics. This differs from the classic system, where human evaluation was a more significant factor in evaluating housing prices. The CCAO’s website states that [“mass appraisal is a way to put fairness into the assessment system.”](https://www.cookcountyassessor.com/index.php/about-cook-county-assessors-office) \n",
    "\n",
    "The dataset we’re currently working with is the same one used for mass appraisal. Let’s examine the column `Neighborhood Code` and see how the location of homes might influence the fairness of mass appraisal.\n",
    " \n",
    "## HCE: Question 2\n",
    "\n",
    "How does mass appraisal appeal to fairness? Consider how it might both benefit and hurt homeowners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Neighborhood vs Sale Price\n",
    "\n",
    "First, let's take a look at the relationship between neighborhood and sale prices of the houses in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2)\n",
    "\n",
    "sns.boxplot(\n",
    "    x='Neighborhood',\n",
    "    y='SalePrice',\n",
    "    data=training_data.sort_values('Neighborhood'),\n",
    "    ax=axs[0]\n",
    ")\n",
    "\n",
    "sns.countplot(\n",
    "    x='Neighborhood',\n",
    "    data=training_data.sort_values('Neighborhood'),\n",
    "    ax=axs[1]\n",
    ")\n",
    "\n",
    "# Draw median price\n",
    "axs[0].axhline(\n",
    "    y=training_data['SalePrice'].median(), \n",
    "    color='red',\n",
    "    linestyle='dotted'\n",
    ")\n",
    "\n",
    "# Label the bars with counts\n",
    "for patch in axs[1].patches:\n",
    "    x = patch.get_bbox().get_points()[:, 0]\n",
    "    y = patch.get_bbox().get_points()[1, 1]\n",
    "    axs[1].annotate(f'{int(y)}', (x.mean(), y), ha='center', va='bottom')\n",
    "    \n",
    "# Format x-axes\n",
    "axs[1].set_xticklabels(axs[1].xaxis.get_majorticklabels(), rotation=90)\n",
    "axs[0].xaxis.set_visible(False)\n",
    "\n",
    "# Narrow the gap between the plots\n",
    "plt.subplots_adjust(hspace=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1a <a name=\"q1a\"></a> \n",
    "\n",
    "Based on the plot above, what can be said about the relationship between the houses' sale prices and their neighborhoods?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "points: 1\n",
    "manual: True\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1b <a name=\"q1b\"></a> \n",
    "\n",
    "One way we can deal with the lack of data from some neighborhoods is to create a new feature that bins neighborhoods together.  Let's categorize our neighborhoods in a crude way: we'll take the top 3 neighborhoods measured by median `SalePrice` and identify them as \"rich neighborhoods\"; the other neighborhoods are not marked.\n",
    "\n",
    "Write a function that returns list of the top n most pricy neighborhoods as measured by our choice of aggregating function.  For example, in the setup above, we would want to call `find_rich_neighborhoods(training_data, 3, np.median)` to find the top 3 neighborhoods measured by median `SalePrice`.\n",
    "\n",
    "*The provided tests check that you answered correctly, so that future analyses are not corrupted by a mistake.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rich_neighborhoods(data, n=3, metric=np.median):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): should contain at least a string-valued Neighborhood\n",
    "        and a numeric SalePrice column\n",
    "      n (int): the number of top values desired\n",
    "      metric (function): function used for aggregating the data in each neighborhood.\n",
    "        for example, np.median for median prices\n",
    "    \n",
    "    Output:\n",
    "      a list of the top n richest neighborhoods as measured by the metric function\n",
    "    \"\"\"\n",
    "    neighborhoods = ...\n",
    "    return neighborhoods\n",
    "\n",
    "rich_neighborhoods = find_rich_neighborhoods(training_data, 3, np.median)\n",
    "rich_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1c <a name=\"q1c\"></a> \n",
    "\n",
    "We now have a list of neighborhoods we've deemed as richer than others.  Let's use that information to make a new variable `in_rich_neighborhood`.  Write a function `add_rich_neighborhood` that adds an indicator variable which takes on the value 1 if the house is part of `rich_neighborhoods` and the value 0 otherwise.\n",
    "\n",
    "**Hint:** [`pd.Series.astype`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Series.astype.html) may be useful for converting True/False values to integers.\n",
    "\n",
    "*The provided tests check that you answered correctly, so that future analyses are not corrupted by a mistake.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_in_rich_neighborhood(data, neighborhoods):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): a data frame containing a 'Neighborhood' column with values\n",
    "        found in the codebook\n",
    "      neighborhoods (list of strings): strings should be the names of neighborhoods\n",
    "        pre-identified as rich\n",
    "    Output:\n",
    "      data frame identical to the input with the addition of a binary\n",
    "      in_rich_neighborhood column\n",
    "    \"\"\"\n",
    "    data['in_rich_neighborhood'] = ...\n",
    "    return data\n",
    "\n",
    "rich_neighborhoods = find_rich_neighborhoods(training_data, 3, np.median)\n",
    "training_data = add_in_rich_neighborhood(training_data, rich_neighborhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having identified rich neighborhoods, let’s now look toward the other end of the spectrum: lower-valued properties. According to the CCAO, their assessment system struggles with accurately predicting the values of properties that are worth less than 150k because they lack data for those properties. This ultimately diminishes the usefulness of mass appraisal for these properties. \n",
    "\n",
    "## HCE: Question 2.5\n",
    "\n",
    "In what situation does mass appraisal fail? How might mass appraisal work unfairly in this way?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Floodplain\n",
    "\n",
    "In 2019, the Cook County Assessor’s Office added the Federal Emergency Management Agency’s [floodplain data](https://msc.fema.gov/portal/home) to its assessment models. As described in their [Medium article](https://medium.com/@AssessorCook/why-and-how-floodplain-data-is-used-in-cook-county-property-assessments-6269d75189d7), “a floodplain is an area near a body of water that has a high risk of flooding.” A value of 0 indicates that a property is not on a floodplain, while a value of 1 indicates that a property is on a floodplain.\n",
    "\n",
    "### Question 2a <a name=\"q2a\"></a>\n",
    "\n",
    "Let's see if our data set has any missing values.  Create a Series object containing the counts of missing values in each of the columns of our data set, sorted from greatest to least.  The Series should be indexed by the variable names.  For example, `missing_counts['Floodplain']` should return 975.\n",
    "\n",
    "**Hint:** [`pandas.DataFrame.isnull`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html) may help here.\n",
    "\n",
    "*The provided tests check that you answered correctly, so that future analyses are not corrupted by a mistake.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = ...\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following values (or lack thereof) thus have the following meanings:\n",
    "```\n",
    "Floodplain (Ordinal): Whether the property is on a floodplain\n",
    "\n",
    "       0\tNot on floodplain\n",
    "       1\tOn floodplain\n",
    "       Null\tNo data\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2b <a name=\"q2b\"></a>\n",
    "\n",
    "A missing value here means that there is no data for the property.  Let's fix this in our data set.  Write a function that replaces the missing values in `Floodplain` with `'No data'`.  In addition, it should replace each abbreviated condition with its full word.  For example, `'1'` should be changed to `'On floodplain'`.  Hint: the [DataFrame.replace](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html) method may be useful here.\n",
    "\n",
    "*The provided tests check that part of your answer is correct, but they are not fully comprehensive.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_floodplain(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): a data frame containing a Floodplain column.  Its values\n",
    "                         should be limited to those found in the codebook\n",
    "    Output:\n",
    "      data frame identical to the input except with a refactored Floodplain column\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return data\n",
    "    \n",
    "training_data = fix_floodplain(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2b\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['Floodplain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Important Note on One Hot Encoding <a name=\"important_note\"></a>\n",
    "\n",
    "Unfortunately, simply fixing these missing values isn't sufficient for using `Floodplain` in our model.  Since `Floodplain` is a categorical variable, we will have to one-hot-encode the data using `DictVectorizer` from Lab 6. Note that we dropped the first one-hot-encoded column. For more information on categorical data in pandas, refer to this [link](https://pandas-docs.github.io/pandas-docs-travis/categorical.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_floodplain(data):\n",
    "    \"\"\"\n",
    "    One-hot-encodes floodplain.  New columns are of the form Floodplain=STATUS\n",
    "    \"\"\"\n",
    "    vec_enc = DictVectorizer()\n",
    "    vec_enc.fit(data[['Floodplain']].to_dict(orient='records'))\n",
    "    floodplain_data = vec_enc.transform(data[['Floodplain']].to_dict(orient='records')).toarray()\n",
    "    floodplain_cats = vec_enc.get_feature_names()\n",
    "    floodplain = pd.DataFrame(floodplain_data, columns=floodplain_cats)\n",
    "    data = pd.concat([data, floodplain], axis=1)\n",
    "    data = data.drop(columns=floodplain_cats[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ohe_floodplain(training_data)\n",
    "training_data.filter(regex='Floodplain').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Improved Linear Models\n",
    "\n",
    "In this section, we will create linear models that produce more accurate estimates of the housing prices in Ames than the model created in Homework 5, but at the expense of increased complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Adding Covariates to our Model\n",
    "\n",
    "It's finally time to fit our updated linear regression model using the ordinary least squares estimator! Our new model consists of the linear model from Homework 5, with the addition of the our newly created `in_rich_neighborhood` variable and our one-hot-encoded fireplace quality variables:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{SalePrice} & = \\theta_0 + \\theta_1 \\cdot \\text{Gr_Liv_Area} + \\theta_2 \\cdot \\text{Garage_Area} + \n",
    "\\theta_3 \\cdot \\text{TotalBathrooms} + \\theta_4 \\cdot \\text{in_rich_neighborhood} + \\\\\n",
    "& \\quad \\: \\theta_5 \\cdot \\text{Floodplain=On floodplain} + \\theta_6 \\cdot \\text{Floodplain=No data}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a <a name=\"q3a\"></a>\n",
    "\n",
    "Although the floodplain variable that we explored in Question 2 has three categories, only two of these categories' indicator variables are included in our model. Is this a mistake, or is it done intentionally? Why?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "points: 1\n",
    "manual: True\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b <a name=\"q3b\"></a>\n",
    "\n",
    "We still have a little bit of work to do prior to esimating our linear regression model's coefficients. Instead of having you go through the process of selecting the pertinent convariates and creating a [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object for our linear model again, we will provide the necessary code from Homework 5. However, we will now use cross validation to help validate our model instead of explicitly splitting the data into a training and testing set.\n",
    "\n",
    "First, we will re-import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"ccao_train_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement a reusable pipeline that selects the required variables in our data and splits our covariates and response variable into a matrix and a vector, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(data, *columns):\n",
    "    \"\"\"Select only columns passed as arguments.\"\"\"\n",
    "    return data.loc[:, columns]\n",
    "\n",
    "def process_data_gm(data):\n",
    "    \"\"\"Process the data for a guided model.\"\"\"\n",
    "    # One-hot-encode fireplace quality feature\n",
    "    data = fix_fireplace_qu(data)\n",
    "    data = ohe_fireplace_qu(data)\n",
    "    \n",
    "    # Use rich_neighborhoods computed earlier to add in_rich_neighborhoods feature\n",
    "    data = add_in_rich_neighborhood(data, rich_neighborhoods)\n",
    "    \n",
    "    # Transform Data, Select Features\n",
    "    data = select_columns(data, \n",
    "                          'SalePrice', \n",
    "                          'Gr_Liv_Area', \n",
    "                          'Garage_Area',\n",
    "                          'TotalBathrooms',\n",
    "                          'in_rich_neighborhood',\n",
    "                          'Floodplain=On floodplain',\n",
    "                          'Floodplain=No data'\n",
    "                         )\n",
    "    \n",
    "    # Return predictors and response variables separately\n",
    "    X = data.drop(['SalePrice'], axis = 1)\n",
    "    y = data.loc[:, 'SalePrice']\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split our dataset into training and testing sets using our data cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the training data\n",
    "# Our functions make this very easy!\n",
    "X_train, y_train = process_data_gm(training_data)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initialize a [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object as our linear model. We set the `fit_intercept=True` to ensure that the linear model has a non-zero intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm\n",
    "\n",
    "linear_model = lm.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "After a little bit of work, it's finally time to fit our updated linear regression model. Use the cell below to estimate the model, and then use it to compute the fitted value of `SalePrice` over the training data.\n",
    "\n",
    "*The provided tests check that you answered correctly, so that future analyses are not corrupted by a mistake.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model below\n",
    "\n",
    "# Compute the fitted and predicted values of SalePrice\n",
    "y_fitted = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that, as we consider more features in our model, its computational complexity grows. It isn’t just computational, however - increased complexity requires greater expertise and understanding of data science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3c <a name=\"q3c\"></a>\n",
    "\n",
    "Let's assess the performance of our new linear regression model using the Root Mean Squared Error function that we created in Homework 5.\n",
    "\n",
    "$$RMSE = \\sqrt{\\dfrac{\\sum_{\\text{houses}}(\\text{actual price for house} - \\text{predicted price for house})^2}{\\text{# of houses}}}$$\n",
    "\n",
    "The function is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculates RMSE from actual and predicted values\n",
    "    Input:\n",
    "      predicted (1D array): vector of predicted/fitted values\n",
    "      actual (1D array): vector of actual values\n",
    "    Output:\n",
    "      a float, the root-mean square error\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Please compute the training error using the `rmse` function above.\n",
    "\n",
    "*The provided tests for this question do not confirm that you have answered correctly; only that you have assigned each variable to a non-negative number.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = ...\n",
    "print(\"Training RMSE: {}\".format(training_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slighlty modified version of the `cross_validate_rmse` function from Lecture 18 is also provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "def cross_validate_rmse(model, X, y):\n",
    "    model = clone(model)\n",
    "    five_fold = KFold(n_splits=5)\n",
    "    rmse_values = []\n",
    "    for tr_ind, va_ind in five_fold.split(X):\n",
    "        model.fit(X.iloc[tr_ind,:], y.iloc[tr_ind])\n",
    "        rmse_values.append(rmse(y.iloc[va_ind], model.predict(X.iloc[va_ind,:])))\n",
    "    return np.mean(rmse_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now use the `cross_validate_rmse` functions to calculate the cross validation error in the cell below.\n",
    "\n",
    "*The provided tests for this question do not confirm that you have answered correctly; only that you have assigned each variable to a non-negative number.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3d\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_error = ...\n",
    "print(\"Cross Validation RMSE: {}\".format(cv_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Open-Response\n",
    "\n",
    "The following part is purposefully left nearly open-ended.  The Cook County data in your possession comes from a larger data set.  Your goal is to provide a linear regression model that accurately predicts the prices of the held-out homes, measured by root mean square error. \n",
    "\n",
    "$$RMSE = \\sqrt{\\dfrac{\\sum_{\\text{houses in public test set}}(\\text{actual price for house} - \\text{predicted price for house})^2}{\\text{# of houses}}}$$\n",
    "\n",
    "Perfect prediction of house prices would have a score of 0, so you want your score to be as low as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Scheme\n",
    "\n",
    "Your grade for Question 4 will be based on your training RMSE and test RMSE. The thresholds are as follows:\n",
    "\n",
    "Points | 3 | 2 | 1 | 0\n",
    "--- | --- | --- | --- | ---\n",
    "Training RMSE | Less than 36k | 36k - 38k | 38k - 40k | More than 40k\n",
    "\n",
    "Points | 3 | 2 | 1 | 0\n",
    "--- | --- | --- | --- | ---\n",
    "Test RMSE | Less than 37k | 37k - 40k | 40k - 43k | More than 43k\n",
    "\n",
    "\n",
    "### One Hot Encoding\n",
    "\n",
    "If you choose to include more categorical features in your model, you'll need to one-hot-encode each one. Remember that if a categorical variable has a unique value that is present in the training set but not in the test set, one-hot-encoding this variable will result in different outputs for the training and test sets (different numbers of one-hot columns). Watch out for this! Feel free to look back at how we [one-hot-encoded `Floodplain`](#important_note).\n",
    "\n",
    "To generate all possible categories for a categorical variable, we suggest reading through a more detailed description of each variable [here](https://datacatalog.cookcountyil.gov/Property-Taxation/Cook-County-Assessor-s-Residential-Sales-Data/5pge-nu6u) or finding the values programmatically across both the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Your Own Linear Model <a name=\"q4\"></a>\n",
    "\n",
    "Just as in the guided model above, you should encapsulate as much of your workflow into functions as possible. Below, we have initialized `final_model` for you. Your job is to select better features and define your own feature engineering pipeline in `process_data_fm`. We recommend using cross validation to help inform your feature selection process.\n",
    "\n",
    "To evaluate your model, we will process training data using your `process_data_fm`, fit `final_model` with this training data, and compute the training RMSE. Then, we will process the test data with your `process_data_fm`, use `final_model` to predict sale prices for the test data, and compute the test RMSE. See below for an example of the code we will run to grade your model:\n",
    "\n",
    "```\n",
    "training_data = pd.read_csv('ccao_train_cleaned.csv')\n",
    "test_data = pd.read_csv('ccao_test_cleaned.csv')\n",
    "\n",
    "X_train, y_train = process_data_fm(training_data)\n",
    "X_test, y_test = process_data_fm(test_data)\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "y_predicted_train = final_model.predict(X_train)\n",
    "y_predicted_test = final_model.predict(X_test)\n",
    "\n",
    "training_rmse = rmse(y_predicted_train, y_train)\n",
    "test_rmse = rmse(y_predicted_test, y_test)\n",
    "```\n",
    "\n",
    "**Note:** It is your duty to make sure that all of your feature engineering and selection happens in `process_data_fm`, and that the function performs as expected without errors. We will **NOT** accept regrade requests that require us to go back and run code that require typo/bug fixes.\n",
    "\n",
    "**Hint:** Some features may have missing values in the test set but not in the training set. Make sure `process_data_fm` handles missing values appropriately for each feature!\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "points: 6\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = lm.LinearRegression(fit_intercept=True) # No need to change this!\n",
    "\n",
    "def process_data_fm(data):\n",
    "    ...\n",
    "    # Return predictors and response variables separately\n",
    "    X = data.drop(['SalePrice'], axis = 1)\n",
    "    y = data.loc[:, 'SalePrice']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q4\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 5: EDA for Feature Selection\n",
    "\n",
    "In the following question, explain a choice you made in designing your custom linear model in Question 4. First, make a plot to show something interesting about the data. Then explain your findings from the plot, and describe how these findings motivated a change to your model.\n",
    "\n",
    "### Question 5a <a name=\"q5a\"></a>\n",
    "\n",
    "In the cell below, create a visualization that shows something interesting about the dataset.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5a\n",
    "points: 2\n",
    "manual: True\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for visualization goes here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5b <a name=\"q5b\"></a>\n",
    "\n",
    "Explain any conclusions you draw from the plot above, and describe how these conclusions affected the design of your model. After creating the plot, did you add/remove certain features from your model, or did you perform some other type of feature engineering? How significantly did these changes affect your rmse?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5b\n",
    "points: 2\n",
    "manual: True\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in Modeling \n",
    "\n",
    "As the previous question highlighted, a series of decisions are built into your model: When the goal is to minimize rmse (and receive full homework points!), it makes sense to use features that carry a lot of weight. These choices, however, create a representation of reality - intentional or not - and dictate what a model considers valuable when generating any type of prediction. \n",
    "Having worked through a structured data analysis/modeling process and built your own model, we’ll now take a step back and look at how your work fits into the real world. Because civic data initiatives have multiple stakeholders, it’s imperative to understand how different groups interact with particular aspects of the data. For a student, it’s important to minimize a model’s error for the sake of completing this assignment and learning basic data science principles. For the CCAO, it’s important to project a vision of fairness for housing assessments in order to maintain the trust of Cook County’s constituents. An assessment model has the potential to reach stakeholders beyond the assessor’s office, so let’s explore this further!\n",
    " \n",
    " \n",
    "## HCE: Question 3\n",
    "Above, we included `Floodplain` in our model to predict property values. This column could be useful to organizations outside of the CCAO - insurance companies, for example, calculate risk and insure houses based on their characteristics. If an insurance company wanted to assign an insurance rate to a house, why might they use `Floodplain` from the county assessor as part of their calculation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HCE: Question 4\n",
    "This dataset, and many like it around the country, are available free or for sale to any business. What kinds of businesses and industries would be interested in the feature(s) you identified? How could a business or industry use the feature(s) to make a decision that would help them? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HCE: Question 5 \n",
    "In the cell below, generate a visualization of the feature(s) you selected which could help the business or industry use this data to make a decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HCE: Question 6\n",
    "While you have argued that this feature could help a given business make an informed decision, is there a potential for the feature(s) to mislead them? How so? \n",
    "\n",
    "*Hint:* If you’re stuck, consider the limitations of bias in data collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your work through these questions demonstrates, your features and modeling process have relevance to a diversity of fields. You may have noticed, however, that the quality of this information is contingent on its explainability, i.e. what a particular variable would mean or represent to different industries. This need for context and clarity not only reveals the underlying biases of our work but also characterizes another common approach to fairness: transparency.\n",
    " \n",
    "## HCE: Question 7\n",
    "What would you consider a transparent and fair process in regard to home assessment, and how might you implement these ideas in your modeling process?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Approaching Fairness through Transparency\n",
    "\n",
    "\n",
    "In this homework, the validity of your model’s assessments is determined by how closely your test set’s results align with the CCAO’s residential sales dataset. The CCAO, however, does not have an autograder to check its work; its predictions, after all, draw on this dataset to re-establish the standard for fair and accurate property values throughout the triennial assessment period. Instead, the Office champions transparency as a guiding principle in regard to fairness. \n",
    "\n",
    "\n",
    "### Transparency and the CCAO\n",
    "\n",
    "After a lawsuit was filed against the CCAO for producing [“racially discriminatory assessments and taxes,\"](https://harris.uchicago.edu/news-events/news/prof-chris-berry-testifies-institutional-racism-cook-county-property-taxes) the Office decided to tackle these inequities by committing to transparency initiatives. The hope was that, by exposing the inner workings of the CCAO’s property valuation process, their assessment results could be publicly verified as accurate and therefore trusted to be fair. \n",
    "\n",
    "These transparency initiatives include publishing all of the CCAO’s work on [GitLab](https://gitlab.com/ccao-data-science---modeling). By allowing the public to access any updates to the system in real-time, the Office argues that they increase accessibility to a process that had previously been blackboxed - obscured and hidden - from the public. Empowered by transparency, the citizens of Cook County would ideally hold the Assessor’s Office accountable for their work, redistributing the balance of power between the CCAO and its constituents. And in this scenario, this form of transparency would thus contribute to the legitimacy and fairness of the Office’s property assessments.\n",
    "\n",
    "Additionally, these measures were, in part, developed to push back against the inequities of the tax lawyer industry. Because hiring a tax lawyer to negotiate for lower valuations (and therefore taxes) is limited to the wealthy, property owners with a lower socioeconomic status paid a disproportionate amount of tax. However, because the CCAO’s assessment process is now public, tax lawyers can only contest the CCAO’s work through technical means - in other words, tax lawyers must abide by the rules set by the CCAO. In this way, the transparency initiatives aim to shift the balance of power in property assessment and taxes away from the tax lawyer industry. \n",
    "\n",
    "## HCE: Question 8\n",
    "\n",
    "How do the CCAO’s transparency initiatives aim to redistribute power between the tax lawyer industry, the CCAO, and the constituents of Cook County?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these balances of power in mind, the next step is to critically examine the effectiveness and reach of the CCAO’s transparency initiatives. We can assess them in three ways: \n",
    "\n",
    "* Accessibility - Are there barriers or limits to participating in the transparency initiatives implemented by the CCAO? More specifically, who can and cannot interact with and understand the CCAO’s published code on GitLab?\n",
    "* Explainability - What efforts has the CCAO made to effectively communicate information about their assessment process? To what extent do they elaborate on the documentation of their GitLab repository?\n",
    "* Accountability - In what ways can the CCAO be held accountable through their transparency initiatives? Are there barriers to who can hold them accountable?\n",
    "\n",
    "As you may have noticed, these terms are closely linked to one another. We’ll now examine these standards for transparency by diving deeper into the CCAO’s assessment process. \n",
    "\n",
    "According to the CCAO’s [Progress Report on Implementation of 100 Day Objectives](https://gitlab.com/ccao-data-science---modeling/ccao_sf_cama_dev/-/blob/master/documentation/Progress%20Report%20on%20Implementation%20of%20100%20Day%20Objectives.pdf), their  assessment system operates beyond a single model or series of models: It allows “any number of models to be specified and runs all of them (...) Each model is subjected to a battery of tests as outlined by the International Association of Assessing Officers. The algorithm then recommends a model based on its performance on these tests.” This complex modeling, coupled with real estate-based data collection practices, enable the CCAO to leverage their data scientists’ expertise in their final assessments. It’s important to acknowledge this power dynamic because it directly interacts with the notion of transparency in the assessment system. \n",
    "\n",
    "Let’s start with accessibility. To be frank, the CCAO’s algorithmic modeling system is almost completely inaccessible to the everyday person, despite its viewability on GitLab. Understanding code - much less algorithms and statistical models written in code - is a high barrier to entry, severely restricting accessibility on the basis of expertise. This is where the next metric for transparency, explainability, comes in. \n",
    "\n",
    "## HCE: Question 9 \n",
    "\n",
    "Take a look at the Residential Automated Valuation Model files under the Models subgroup in the CCAO’s [GitLab](https://gitlab.com/ccao-data-science---modeling). Without directly looking at any code, does the documentation sufficiently explain how the residential valuation model works? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the barrier of expertise, explanations are perhaps the only way that the CCAO can bridge the technical gaps throughout the assessment process. However, as you might’ve noticed, these explanations are also lacking. That leaves the final piece of the puzzle - the ideal product of transparency, accountability.\n",
    "\n",
    "Without any measures to expand the accessibility and explainability of their algorithmic modeling, the prospects of holding the CCAO accountable for their assessments are dim. The level of expertise needed to check the CCAO’s systems excludes participatory voices from the community. Even community members who have valuable insights into measures of inequity - itself another form of expertise - cannot engage with technical expertise in data and modeling.\n",
    "\n",
    "WIth all that said, it’s imperative to acknowledge that the CCAO is a government institution. Although their work relies on technical expertise, anyone who wishes to critique - or even just interact with - the assessment pipeline should have the means to; all of Cook County’s constituents are affected by property assessments, and they should not be required to have a technical background in order to participate in a technical space. \n",
    "\n",
    "## HCE: Question 10 \n",
    "\n",
    "In what ways does the CCAO’s implementation of transparency fail? What aspects of it are inaccessible and to whom? Consider the concepts of expertise and power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what role does transparency really play in relation to fairness? Given its limitations in regard to accessibility, the CCAO’s transparency initiatives can come across as merely performative. That said, they certainly still demonstrate the Office’s commitment to fairness and equity, performing the social function of instilling faith in its work. And while there is validity in fostering trust with the local community as a governing institution, it’s nonetheless vital to understand what this faith is built on: The CCAO operates its assessment model by leveraging its expertise, and transparency is then used to reinforce the legitimacy of the CCAO’s work by exemplifying a gesture of good will. And through this process, the CCAO’s power to determine property assessments and taxes is thus maintained through this process. \n",
    "\n",
    "## HCE: Question 11\n",
    "\n",
    "How does the CCAO maintain its power in housing assessments and its open-data initiative? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` *Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairness, in the end, is as difficult to define as it is to implement. What you make of this process - from the ingrained bias throughout the data lifecycle to the role of transparency in the CCAO’s work - is for your consideration. Regardless, it raises several challenging but pressing questions: How can we envision fair and equitable data science beyond transparency? How can we go one step further and incorporate justice in data science? \n",
    "\n",
    "The Cook County Assessor’s Office is just one case study. Although it is unique in being the first Assessor’s Office to publicly publish its assessment model and data, it is not exempt from the same scrutiny and critique that other public and private institutions receive. Consider how expertise serves as the baseline and standard for fairness - and consider how data science becomes authoritative despite (or, perhaps, because of) its many exceptions, parameters, and metrics. Continue to reflect on these concepts in your data science work as you learn more and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit\n",
    "\n",
    "Make sure that if you run Kernel > Restart & Run All, your notebook produces the expected outputs for each cell. Congratulations on finishing the assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**\n",
    "\n",
    "<!-- EXPECT 4 EXPORTED QUESTIONS -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "import jassign.to_pdf\n",
    "jassign.to_pdf.generate_pdf('hw6.ipynb', 'hw6.pdf')\n",
    "ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
